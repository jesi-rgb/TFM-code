{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0a64c66-fec9-4176-ab56-827c9a0ba824",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38cd36a3-873b-4230-b804-b3c74b801f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedTextDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, path_file, truncate=False, gpt2_type=\"gpt2\", max_length=768):\n",
    "\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
    "        self.comments = []\n",
    "        \n",
    "        with open(path_file) as file:\n",
    "            comments = file.readlines()\n",
    "            \n",
    "            self.comments = [self.comment_to_tensor(c, max_length) for c in comments]\n",
    "            \n",
    "                \n",
    "        if truncate:\n",
    "            self.comments = self.comments[:20000]\n",
    "        self.comments_count = len(self.comments)\n",
    "        \n",
    "        \n",
    "    def comment_to_tensor(self, comment, max_length):\n",
    "        return torch.tensor(self.tokenizer.encode(comment[:max_length]))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.comments_count\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.comments[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PRUEBA = '../tagged_files/train_tagged_prueba.txt'\n",
    "train_p_dataset = MedTextDataset(DATA_TRAIN_PRUEBA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "len(train_p_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n",
    "    if packed_tensor is None:\n",
    "        return new_tensor, True, None\n",
    "    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n",
    "        return packed_tensor, False, new_tensor\n",
    "    else:\n",
    "        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n",
    "        return packed_tensor, True, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataset,\n",
    "    model,\n",
    "    batch_size=16,\n",
    "    epochs=4,\n",
    "    lr=2e-5,\n",
    "    max_seq_len=400,\n",
    "    warmup_steps=5000,\n",
    "    gpt2_type=\"gpt2\",\n",
    "    device=\"cuda\",\n",
    "    output_dir=\".\",\n",
    "    output_prefix=\"medtex\",\n",
    "    test_mode=False,\n",
    "    save_model_on_epoch=False,\n",
    "):\n",
    "\n",
    "    acc_steps = 100\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    accumulating_batch_count = 0\n",
    "    input_tensor = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(f\"Training epoch {epoch}\")\n",
    "        for idx, entry in tqdm(enumerate(train_dataloader)):\n",
    "            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)\n",
    "\n",
    "            if carry_on and idx != len(train_dataloader) - 1:\n",
    "                continue\n",
    "\n",
    "            input_tensor = input_tensor.to(device)\n",
    "            outputs = model(input_tensor, labels=input_tensor)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "\n",
    "            if (accumulating_batch_count % batch_size) == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                model.zero_grad()\n",
    "\n",
    "            accumulating_batch_count += 1\n",
    "            input_tensor = None\n",
    "        if save_model_on_epoch:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(output_dir, f\"{output_prefix}-{epoch}.pt\"),\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_type = \"gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "0it [00:00, ?it/s]Training epoch 0\n",
      "9it [00:28,  3.22s/it]\n",
      "0it [00:00, ?it/s]Training epoch 1\n",
      "9it [00:31,  3.53s/it]\n",
      "0it [00:00, ?it/s]Training epoch 2\n",
      "9it [00:30,  3.39s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = train(\n",
    "    train_p_dataset,\n",
    "    GPT2LMHeadModel.from_pretrained(gpt2_type),\n",
    "    batch_size=64,\n",
    "    epochs=3,\n",
    "    lr=2e-5,\n",
    "    max_seq_len=140,\n",
    "    warmup_steps=200,\n",
    "    gpt2_type=gpt2_type,\n",
    "    device=\"cpu\",\n",
    "    output_dir=\"trained_models\",\n",
    "    output_prefix=\"medtext\",\n",
    "    save_model_on_epoch=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    entry_count=10,\n",
    "    entry_length=100,\n",
    "    top_p=0.8,\n",
    "    temperature=1.,\n",
    "):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    generated_num = 0\n",
    "    generated_list = []\n",
    "\n",
    "    filter_value = -float(\"Inf\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for entry_idx in trange(entry_count):\n",
    "\n",
    "            entry_finished = False\n",
    "\n",
    "            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "\n",
    "            # Using top-p (nucleus sampling): https://github.com/huggingface/transformers/blob/master/examples/run_generation.py\n",
    "\n",
    "            for i in range(entry_length):\n",
    "                outputs = model(generated, labels=generated)\n",
    "                loss, logits = outputs[:2]\n",
    "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(\n",
    "                    F.softmax(sorted_logits, dim=-1), dim=-1\n",
    "                )\n",
    "\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                    ..., :-1\n",
    "                ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = filter_value\n",
    "\n",
    "                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "                generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "                if next_token in tokenizer.encode(\"<|EOS|>\"):\n",
    "                    entry_finished = True\n",
    "\n",
    "                if entry_finished:\n",
    "\n",
    "                    generated_num = generated_num + 1\n",
    "\n",
    "                    output_list = list(generated.squeeze().numpy())\n",
    "                    output_text = tokenizer.decode(output_list)\n",
    "\n",
    "                    generated_list.append(output_text)\n",
    "                    break\n",
    "            \n",
    "            if not entry_finished:\n",
    "                output_list = list(generated.squeeze().numpy())\n",
    "                output_text = f\"{tokenizer.decode(output_list)}<|EOS|>\" \n",
    "                generated_list.append(output_text)\n",
    "                \n",
    "    return generated_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 10/10 [00:50<00:00,  5.01s/it]\n"
     ]
    }
   ],
   "source": [
    "generated_tweets = generate(model.to(\"cpu\"), GPT2Tokenizer.from_pretrained(gpt2_type), \"<|BOS|>\", entry_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[\"<|BOS|> (but still quite complex) * To me, this whole project is rather simple: Let's figure out what the three top boxes are about. Now, we have to draw the bases of each in our basic 3D cube: Point-by-point.\\n\\n* Point-by-point construction of triangular bases * Point-by-point construction of black box bases * Point-by-point construction of hard box bases * Point-by-point construction of base shapes (We could write<|EOS|>\",\n",
       " '<|BOS|>=-|',\n",
       " '<|BOS|>0.1|',\n",
       " \"<|BOS|>/r0 -z '<\",\n",
       " '<|BOS|>|',\n",
       " '<|BOS|>|',\n",
       " '<|BOS|>--|',\n",
       " '<|BOS|> the product value $O:\\n\\nI can\\'t help but think that maybe the functional side (such as interface passing or parsing) could have done this and probably replaced something like:\\n\\n-- ; Promise wrapping the $O -> event. pipe ([]( \" request \", function (){ return Promise. create ( this, $ O ); });\\n\\nI just don\\'t see how that could\\'ve looked any better, and I\\'m sure some people are still confused about this \"feature\" and<|EOS|>',\n",
       " '<|BOS|> =|',\n",
       " '<|BOS|>BOS']"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "generated_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}